{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3726bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from IPython.display import IFrame\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import activatie_functies\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from netwerk import Netwerk\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a843b0",
   "metadata": {},
   "source": [
    "## P1.1, 1.3, 1.4: opzet netwerk\n",
    "\n",
    "Voor het netwerk heb ik gekozen om in plaats van elke neuron een object te maken, gewoon alle weights en biases in matrices en vectoren te zetten, respectievelijk. Een netwerk evalueren aan de hand van een input vector werkt als volgt:\n",
    "\n",
    "$${(1) \\quad g(x):= f^{L}(W^{L}f^{L-1}(W^{L-1}\\cdots f^{1}(x \\cdot W^{1}+b^{1})\\cdots )+b^{L-1})+b^{L})}$$\n",
    "waarbij:\n",
    " - $x:$ input vector\n",
    " - $L:$ aantal layers\n",
    " - $W^{l}=(w_{jk}^{l}):$ matrix van weights tussen layer $l$ en $l-1$, waarbij $w_{jk}^{l}$ de weight tussen node $j$ in layer $l$ en node $k$ in layer $l-1$ \n",
    " - $b^{l}:$ bias vector van layer $l$\n",
    " - $f^{l}(x):$ activatiefunctie van layer $l$ (in ons model overal hetzelfde)\n",
    " \n",
    "dit kunnen wij versimpelen door de bias vector aan het respectievelijke weight matrix te plakken en een 1 aan de input; een  bias is immers gewoon een weight die altijd geactiveerd wordt.\n",
    "het netwerk wordt dus geëvalueert dmv:\n",
    "\n",
    "$${(2) \\quad h(x):= f^{L}((W^{L}|b^{L})f^{L-1}((W^{L-1}|b^{L-1})\\cdots f^{1}((x|1)(W^{1}|b^{1}))\\cdots )))}$$\n",
    " \n",
    " - $|$ is voor de matrix niet een gewone concatenatie; de bias vector wordt er als het ware onder geplakt.\n",
    " \n",
    "toelichting: de activatie van een Neuron kan worden samengevat als de activatiefunctie over de som van de inputs vermenigvuldigd met de bijhorende weights. Dit laatste deel kan worden gerepresenteerd door de volgende vector multiplicatie (voorbeeld één neuron met twee inputs):\n",
    "\n",
    "$$ (3) \\quad \\begin{bmatrix}\n",
    "x_1 & x_2 & 1 \n",
    "\\end{bmatrix}  \\cdot\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "b \n",
    "\\end{bmatrix} = [x_1 \\cdot w_1 + x_2 \\cdot w_2 + 1 \\cdot b]  $$\n",
    "\n",
    "deze notatie blijkt zich erg goed te lenen voor een netwerk, gezien we nu een volledige laag als volgt kunnen representeren (voorbeeld een layer met drie van zulke neurons):\n",
    "\n",
    "$$ (4) \\quad \\begin{bmatrix}\n",
    "x_a & x_b & 1 \n",
    "\\end{bmatrix}  \\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{a,c} & w_{a,d} & w_{a,e} \\\\\n",
    "w_{b,c} & w_{b,d} & w_{b,e} \\\\\n",
    "b_c & b_d & b_e \n",
    "\\end{bmatrix}  = \n",
    "\\begin{bmatrix} x_a \\cdot w_{a,c} + x_b \\cdot w_{b,c} + b_c & \n",
    "x_a \\cdot w_{a,d} + x_b \\cdot w_{b,d} + b_d  &\n",
    "x_a \\cdot w_{a,e} + x_b \\cdot w_{b,e} + b_e \\end{bmatrix} $$\n",
    "\n",
    "$ x_a $ en $  x_b $ zijn de input a en b, $w_{x,y}$ is de weight tussen neuron x en y, en $b_x$ is de bias van layer x. Het moet nu duidelijk zijn dat als we de activatiefunctie van elk element van de resulterende vector hebben, we een vector hebben met de activaties van deze layer; dit is het stukje $f^{1}((x|1)(W^{1}|b^{1}))$ uit $(2)$. Deze vector met activaties kunnen we nu doorvoeren door het netwerk door het herhaaldelijk te vermenigvuldigen met de weights om uiteindelijk de het resultaat te krijgen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62202726",
   "metadata": {},
   "source": [
    "## P1.2: perceptron test\n",
    "\n",
    "een perceptron is praktisch gewoon een netwerk met maar een layer met een enkele neuron met de `step` als activatiefunctie.\n",
    "enkele gates als voorbeeld voor de functionaliteit van de perceptron hieronder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabellen aanmaken voor tests\n",
    "table_in_2 = np.array(list(itertools.product([0, 1], repeat=1)))\n",
    "\n",
    "table_in_4   = np.array(list(itertools.product([0, 1], repeat=2)))\n",
    "\n",
    "table_in_8   = np.array(list(itertools.product([0, 1], repeat=3)))\n",
    "\n",
    "table_in_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f78c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = Netwerk(0, 0, 0, 0, activatie_functies.STEP)\n",
    "\n",
    "netw._weights = [np.array([[ 1],\n",
    "                           [ 1],\n",
    "                           [-2]])]\n",
    "print(\"truth table AND\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "netw._weights = [np.array([[ 1],\n",
    "                           [ 1],\n",
    "                           [-1]])]\n",
    "print(\"\\ntruth table OR\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "netw._weights = [np.array([[-1],\n",
    "                           [ 0]])]\n",
    "print(\"\\ntruth table NOT\")\n",
    "for x in table_in_2:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "\n",
    "netw._weights = [np.array([[-1],\n",
    "                           [-1],\n",
    "                           [-1],\n",
    "                           [0]])]\n",
    "print(\"\\ntruth table NOR w/ 3 in\")\n",
    "for x in table_in_8:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "netw._weights = [np.array([[ 1],\n",
    "                           [ 1],\n",
    "                           [ 1],\n",
    "                           [-1]])]\n",
    "print(\"\\ntruth table NAND w/ 3 in\")\n",
    "for x in table_in_8:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "g = netw.visualise_network(mindiam=.5, minlen=3, titel='NAND-gate perceptron', filename='NAND')\n",
    "g.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/NAND.gv.bmp')\n",
    "IFrame('./graphviz_renders/NAND.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5cb67",
   "metadata": {},
   "source": [
    "## P1.5: netwerk test\n",
    "\n",
    "Om het perceptron netwerk te demonstreren hieronder een netwerk met de functionaliteit van een half-adder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#`weights` en `biases` is private dus dit mag eigenlijk niet, maar je zou in het echt toch nooit \n",
    "#zelf de weights en biases zetten\n",
    "\n",
    "netw = Netwerk(0, 0, 0, 0, activatie_functies.STEP)\n",
    "netw._weights = [np.array([[ 1, 1,-1],\n",
    "                           [ 1, 1,-1],\n",
    "                           [-2,-1, 1]]),\n",
    "                 np.array([[ 1, 0],\n",
    "                           [ 0, 1],\n",
    "                           [ 0, 1],\n",
    "                           [-1,-2]])]\n",
    "\n",
    "print(\"truth table XOR\") #een XOR is eigenlijk gewoon het sum gedeelte van een half-adder, dus alleen dat gedeelte van\n",
    "                         #de output van de half-adder nemen resulteert in de truth table van een XOR\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)[0][1]))\n",
    "\n",
    "print(\"\\ntruth table half-adder: \")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "g = netw.visualise_network(mindiam=1.2, minlen=4, titel='Half-adder perceptron netwerk', filename='HalfAdder')\n",
    "g.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/HalfAdder.gv.bmp')\n",
    "IFrame('./graphviz_renders/HalfAdder.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534b9d4",
   "metadata": {},
   "source": [
    "## P2: Perceptron learning rule\n",
    "\n",
    "Voor deze opdracht heb ik de functie `update_trivial` gemaakt, die één enkele layer updated aan de hand van \n",
    "$Δw_j := η (target^{(i)} – output^{(i)}) x_j^{(i)}$\n",
    "Wat in onze enkele layer dus neerkomt op*:\n",
    "$$ (5) \\quad W_1 = W_0 + η (target^{(i)} – f(W_0in^{(i)}))⊗_{outer}in^{(i)}$$ \n",
    "waarbij:\n",
    " - $W_n:$ matrix van weights (incl. biases) na update n \n",
    " - $η:$ learning rate\n",
    " - $target^{(i)}:$ target feature i \n",
    " - $in^{(i)}:$ input feature i (incl. 1 voor bias)\n",
    " - $f(x):$ activatiefunctie\n",
    " \n",
    "toelichting:\n",
    "zoals uitgelegd bij P1, $f(W_0in^{(i)})$ is de vector met activaties van de neuron laag. Als dit van de target af trekken krijgen we een vector met errors van deze laag. We weten dat $Δw_{x,y}$ gelijk is aan de error van neuron y vermenigvuldigd met de 'bijdrage' van de weight van x naar y aan de error -- dus de input-laag. We kunnen nu dus een matrix van $Δw$ opstellen waaruit zal blijken dat dit resulteerd uit een simpele vectormultiplicatie van de input vector en de error vector (voorbeeld een outputlaag met drie neurons):\n",
    "\n",
    "$$ (6) \\quad \\begin{bmatrix}\n",
    "Δw_{a,c} & Δw_{a,d} & Δw_{a,e} \\\\\n",
    "Δw_{b,c} & Δw_{b,d} & Δw_{b,e} \\\\\n",
    "Δb_c & Δb_d & Δb_e \n",
    "\\end{bmatrix} = \n",
    "η\\begin{bmatrix}\n",
    "err_c \\cdot x_a & err_d \\cdot x_a & err_e \\cdot x_a \\\\\n",
    "err_c \\cdot x_b & err_d \\cdot x_b & err_e \\cdot x_b \\\\\n",
    "err_c \\cdot 1 & err_d \\cdot 1 & err_e \\cdot 1\n",
    "\\end{bmatrix} = \n",
    "η\\begin{bmatrix}\n",
    "err_c \\\\\n",
    "err_d \\\\\n",
    "err_e \n",
    "\\end{bmatrix} ⊗\n",
    "\\begin{bmatrix}\n",
    "x_a \\\\\n",
    "x_b \\\\\n",
    "1 \n",
    "\\end{bmatrix} \n",
    "$$ \n",
    "\n",
    "\n",
    " \n",
    "**note: later ben ik erachter gekomen dat dit op hetzelfde neerkomt als $W_0 + η (target^{(i)} – f(W_0in^{(i)}))^T \\cdot (in^{(i)}) $, maar ik dacht ik laat het er in want ik had nog nooit van een outer product gehoord*:\n",
    "\n",
    "$$ (7) \\quad \\begin{bmatrix}\n",
    "err_c \\\\\n",
    "err_d \\\\\n",
    "err_e \n",
    "\\end{bmatrix} ⊗\n",
    "\\begin{bmatrix}\n",
    "x_a \\\\\n",
    "x_b \\\\\n",
    "1 \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "err_c \\\\\n",
    "err_d \\\\\n",
    "err_e \n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "x_a &\n",
    "x_b &\n",
    "1 \n",
    "\\end{bmatrix} = E^T \\cdot in$$\n",
    "\n",
    "dit is overigens ook hetzelfde als $in^T \\cdot E$, deze notatie komt bij backpropegation terug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a93c6c",
   "metadata": {},
   "source": [
    "## P2.3 a: AND \n",
    "hieronder de learning rule gedemonstreerd door een perceptron de AND-gate te leren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285810cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1819772)\n",
    "netw = Netwerk(0, 0, 2, 1, activatie_functies.STEP, .8)\n",
    "\n",
    "d_and = np.array([[0],\n",
    "                  [0],\n",
    "                  [0],\n",
    "                  [1]])\n",
    "\n",
    "print(\"\\ninitial weights:\")\n",
    "for w in netw._weights:\n",
    "    print(w)\n",
    "print(\"initial MSE:\")\n",
    "print(netw.loss_MSE(table_in_4, d_and))\n",
    "print()\n",
    "\n",
    "for i in range(4):\n",
    "    netw.update_trivial(table_in_4, d_and, True)\n",
    "\n",
    "g1 = netw.visualise_network(mindiam=.5, minlen=5, filename='learningRule')\n",
    "g1.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/learningRule.gv.bmp')\n",
    "IFrame('./graphviz_renders/learningRule.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c73b81f",
   "metadata": {},
   "source": [
    "## P2.3 b: XOR\n",
    "een enkele perceptron kan nooit een XOR-gate leren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706068e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1819772)\n",
    "netw = Netwerk(0, 0, 2, 1, activatie_functies.STEP, .8)\n",
    "\n",
    "d_xor = np.array([[0], \n",
    "                  [1], \n",
    "                  [1], \n",
    "                  [0]])\n",
    "\n",
    "print(\"\\ninitial weights:\")\n",
    "for w in netw._weights:\n",
    "    print(w)\n",
    "print(\"initial MSE:\")\n",
    "print(netw.loss_MSE(table_in_4, d_xor))\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    netw.update_trivial(table_in_4, d_xor, True)\n",
    "\n",
    "g1 = netw.visualise_network(mindiam=.5, minlen=5, filename='learningRule')\n",
    "g1.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/learningRule.gv.bmp')\n",
    "IFrame('./graphviz_renders/learningRule.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df215a",
   "metadata": {},
   "source": [
    "## P2.3 c: Iris\n",
    "\n",
    "Zoals in de plot hieronder te zien is er een groot verschil tussen *Setosa* (indigo) en *Versicolour* (geel). De twee groepen zijn volledig te onderscheiden met een rechte lijn in elke dimensie, wat het voor ons perceptron netwerk dus makkelijk maakt.\n",
    "\n",
    "tussen *Versicolour* (geel) en *Verginica* (lochinvar) valt geen recht flak te trekken, en dus zal het ook onmogelijk zijn om een perfecte MSE te behalen.\n",
    "Het duurt wat langer voor het netwerk getraind is, en komt op zn best rond de 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081177ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = np.reshape(iris.target,(-1,1))\n",
    "data = np.c_[X,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbbaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.T[0]\n",
    "y = data.T[2] #dimensie 1 komt voor elke soort het meest overeen, en we kunnen maar 3 dimensies plotten, dus die is weggelaten\n",
    "z = data.T[3] \n",
    "c = data.T[4]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(x, y, z, c=c)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1819772)\n",
    "netw = Netwerk(0, 0, 4, 1, activatie_functies.STEP, .1)\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:100]\n",
    "y = np.reshape(iris.target[:100],(-1,1))\n",
    "\n",
    "for i in range(5):\n",
    "    netw.update_trivial(X, y, False)\n",
    "    print(\"weights: \" + str(netw._weights))\n",
    "    print(\"loss:    \" + str(netw.loss_MSE(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "netw = Netwerk(0, 0, 4, 1, activatie_functies.STEP, .1)\n",
    "\n",
    "X = iris.data[50:]\n",
    "y = np.reshape(iris.target[:100],(-1,1))\n",
    "\n",
    "for i in range(500):\n",
    "    netw.update_trivial(X, y, False)\n",
    "    \n",
    "print(\"weights: \" + str(netw._weights))\n",
    "print(\"loss:    \" + str(netw.loss_MSE(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa66171",
   "metadata": {},
   "source": [
    "## P3.2: Neuron Unit\n",
    "\n",
    "Een neuron is gewoon een perceptron maar dan met een andere activatiefunctie (in ons geval de sigmoid) dan de step. we hoeven dus ook niet vrij veel te veranderen om een neuron te krijgen.\n",
    "\n",
    "zoals hieronder te zien werkt de neuron niet met dezelfde weights als de perceptron, als de uitkomst wordt afgerond zit het er echter wel in de buurt. de Sigmoid komt immers niet exact 0 of 1 uit, maar iets daar tussenin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = Netwerk(0, 0, 0, 0, activatie_functies.SIGMOID)\n",
    "\n",
    "netw._weights = [np.array([[ 1],\n",
    "                           [ 1],\n",
    "                           [-2]])]\n",
    "print(\"truth table AND\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "netw._weights = [np.array([[ 1],\n",
    "                           [ 1],\n",
    "                           [-1]])]\n",
    "print(\"\\ntruth table OR\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "netw._weights = [np.array([[-1],\n",
    "                           [ 0]])]\n",
    "print(\"\\ntruth table NOT\")\n",
    "for x in table_in_2:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6dd42",
   "metadata": {},
   "source": [
    "als we de weights zodanig veranderen dat de uitkomst helemaal links of rechts van de sigmoid zit wordt de uitkomst al correcter. ik heb hier voor meervouden van 6 gekozen, gezien σ(6) = 0.99, wat close enough by de 1 zit voor deze doeleinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = Netwerk(0, 0, 0, 0, activatie_functies.SIGMOID)\n",
    "\n",
    "netw._weights = [np.array([[ 12],\n",
    "                           [ 12],\n",
    "                           [-18]])]\n",
    "print(\"truth table AND\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x).round(3)))\n",
    "    \n",
    "netw._weights = [np.array([[ 12],\n",
    "                           [ 12],\n",
    "                           [ -6]])]\n",
    "print(\"\\ntruth table OR\")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x).round(3)))\n",
    "    \n",
    "netw._weights = [np.array([[-12],\n",
    "                           [  6]])]\n",
    "print(\"\\ntruth table NOT\")\n",
    "for x in table_in_2:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x).round(3)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw._weights = [np.array([[-12],\n",
    "                           [-12],\n",
    "                           [-12],\n",
    "                           [  6]])]\n",
    "\n",
    "print(\"\\ntruth table NOR w/ 3 in\")\n",
    "for x in table_in_8:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x).round(3)))\n",
    "    \n",
    "g = netw.visualise_network(mindiam=.5, minlen=3, titel='NOR-gate neuron', filename='NOR')\n",
    "g.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/NOR.gv.bmp')\n",
    "IFrame('./graphviz_renders/NOR.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = Netwerk(0, 0, 0, 0, activatie_functies.SIGMOID)\n",
    "netw._weights = [np.array([[ 12, 12,-12],\n",
    "                           [ 12, 12,-12],\n",
    "                           [-18, -6, 18]]),\n",
    "                 np.array([[ 12,  0],\n",
    "                           [  0, 12],\n",
    "                           [  0, 12],\n",
    "                           [ -6,-18]])]\n",
    "\n",
    "print(\"\\ntruth table half-adder: \")\n",
    "for x in table_in_4:\n",
    "    print(str(x) + \" -> \" + str(netw.evaluate(x)))\n",
    "    \n",
    "g = netw.visualise_network(mindiam=1.2, minlen=4, titel='Half-adder neuraal netwerk', filename='HalfAdder')\n",
    "g.render(directory='graphviz_renders', view=False)\n",
    "im = Image.open('./graphviz_renders/HalfAdder.gv.bmp')\n",
    "IFrame('./graphviz_renders/HalfAdder.gv.bmp', width=im.size[0], height=im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494e7cb",
   "metadata": {},
   "source": [
    "## P4 - Backpropagation\n",
    "\n",
    "backpropagation is het berekenen van de gradienten voor elke laag door de afgeleide van de loss van rechts naar links door het netwerk heen te voeren.\n",
    "\n",
    "de error van de output-layer wordt gegeven door het Hadamard product tussend de afgeleide van de activatiefunctie te vermenigvuldigen met de afgeleide van de cost-functie:\n",
    "\n",
    "$$(8) \\quad \\delta ^{L}:=f'(z^{L})\\circ C'(a^{L}, y)$$\n",
    "waarbij:\n",
    " - $\\delta ^{l}:$ error layer $l$ \n",
    " - $f(x):$ activatiefunctie\n",
    " - $z^{l}:$ input layer $l$ \n",
    " - $C(x, y):$ costfunctie\n",
    " - $a^{l}:$ activatie layer $l$ \n",
    " - $y:$ target vector\n",
    " - $L:$ totaal aantal layers\n",
    " \n",
    "wat bijvoorbeeld met een sigmoid activatiefunctie en een de squared error als costfunctie er dus zo uit ziet als $\\delta ^{L}= σ(z^{L}) \\circ 1 – σ(z^{L}) \\circ 2(a^{L} - y)$. de factor 2 voor $C'$ wordt in ons model voor simplificatie weggelaten.\n",
    "\n",
    "toelichting:\n",
    "de afgeleide van een functie geeft ons de richtingscoëfficient van de functie op dat punt, als het dus nul is hebben we een lokaal minimum gevonden -- de helling is daar 0. Om onze error te bepalen willen we dus in plaats van gewoon de activatiefunctie van de input ($z$), de afgeleide van de activatiefunctie nemen en de cost functie nemen. Als we een lokaal minimum gevonden hebben zou de error immers 0 moeten zijn.\n",
    "    \n",
    "vervolgens kan deze error van recursief door het netwerk gepropageerd worden door:\n",
    "\n",
    "$$(9) \\quad \\delta ^{l-1}:=f'(z^{l-1})\\circ  (\\delta ^{l} \\cdot (W^{l})^{T})$$\n",
    "\n",
    "toelichting:\n",
    "de vergelijking voor backpropegation voor een enkele neuron die op canvas staat ($\\delta_i = σ'(input_i) \\cdot Σ_j w_{i,j} \\cdot \\delta_j$) kan in vectoren worden uitgedrukt als volgt (voorbeeld laag l en k met beide 3 neurons;  getransposed voor leesbaarheid):\n",
    "\n",
    "$$ (10) \\quad σ'\\begin{bmatrix}  \n",
    "z_{l1}\\\\z_{l2}\\\\z_{l3}\n",
    "\\end{bmatrix}^T \\circ\n",
    "\\begin{bmatrix}  \n",
    "\\delta_{k1} \\cdot w_{l1,k1} + \\delta_{k2} \\cdot w_{l1,k2} + \\delta_{k3} \\cdot w_{l1,k3}\\\\\n",
    "\\delta_{k1} \\cdot w_{l2,k1} + \\delta_{k2} \\cdot w_{l2,k2} + \\delta_{k3} \\cdot w_{l2,k3}\\\\\n",
    "\\delta_{k1} \\cdot w_{l3,k1} + \\delta_{k2} \\cdot w_{l3,k2} + \\delta_{k3} \\cdot w_{l3,k3}\n",
    "\\end{bmatrix}^T$$\n",
    "\n",
    "de rechter vector in $(10)$ valt uit te drukken als de vector-matrix multiplicatie:\n",
    "$$ (11) \\quad \\begin{bmatrix} \n",
    "\\delta_{k1} & \\delta_{k2} & \\delta_{k3}\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} \n",
    "w_{l1,k1} & w_{l2,k1} & w_{l3,k1}\\\\\n",
    "w_{l1,k2} & w_{l2,k2} & w_{l3,k2}\\\\\n",
    "w_{l1,k3} & w_{l2,k3} & w_{l3,k3}  \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "het matrix wat hier rechts is uitgeschreven blijkt hetzelfde te zijn als de transpose van $(4)$. Als we $(4)$, $(10)$ en $(11)$ dus samen nemen krijgen we:\n",
    "\n",
    "$$ (12) \\quad σ'\\begin{bmatrix}  \n",
    "z_{l1}&z_{l2}&z_{l3}\n",
    "\\end{bmatrix} \\circ \n",
    "\\begin{bmatrix} \n",
    "\\delta_{k1} & \\delta_{k2} & \\delta_{k3}\n",
    "\\end{bmatrix} \\cdot \n",
    "\\begin{bmatrix} \n",
    "w_{l1,k1} & w_{l1,k2} & w_{l1,k3}\\\\\n",
    "w_{l2,k1} & w_{l2,k2} & w_{l2,k3}\\\\\n",
    "w_{l3,k1} & w_{l3,k2} & w_{l3,k3}  \n",
    "\\end{bmatrix}^T = (9)\n",
    "$$\n",
    " \n",
    "tot slot wordt de gradient van elke laag berekend door deze errors met de respectievelijke activatie te vermenigvuldigen (dit stuk is vrijwel hetzelfde als P3, zie $(6)$ en $(7)$ voor extra toelichting):\n",
    "$$ (13) \\quad \\nabla_{W^l}C:= (a^{l-1})^{T} \\cdot \\delta ^{l}$$\n",
    "\n",
    "de nieuwe weights zijn dan gegeven door:\n",
    "$$ (14) \\quad W^{l}_{n+1} = W^l_{n} - η \\cdot \\nabla_{W^l}C$$\n",
    "\n",
    "waarbij:\n",
    " - $\\nabla_{W^l}C:$ gradient layer $C$\n",
    " - $W^l_{n}:$ weights van layer $l$, update $n$\n",
    " - $η:$ learning rate\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46602963",
   "metadata": {},
   "source": [
    "### P4.15: AND-poort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "netw = Netwerk(0, 0, 2, 1, activatie_functies.SIGMOID(), 1)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "target = np.array([[0],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [1]])\n",
    "\n",
    "for i in range(1000):\n",
    "    netw.update_backprop(X, target)\n",
    "    \n",
    "print(\"out: \")\n",
    "print(netw.evaluate(X).round(3))\n",
    "print(\"\\nMSE: \")\n",
    "print(netw.loss_MSE(X, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289dd18",
   "metadata": {},
   "source": [
    "### P4.16: XOR-poort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "netw = Netwerk(1, 2, 2, 1, activatie_functies.SIGMOID(), 1)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "target = np.array([[0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]])\n",
    "\n",
    "for i in range(1000):\n",
    "    netw.update_backprop(X, target)\n",
    "    \n",
    "print(\"out: \")\n",
    "print(netw.evaluate(X).round(3))\n",
    "print(\"\\nMSE: \")\n",
    "print(netw.loss_MSE(X, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f054c",
   "metadata": {},
   "source": [
    "### P4.17: Half Adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "netw = Netwerk(1, 3, 2, 2, activatie_functies.SIGMOID(), 1)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "target = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [0, 1],\n",
    "                   [1, 0]])\n",
    "\n",
    "for i in range(1000):\n",
    "    netw.update_backprop(X, target)\n",
    "    \n",
    "print(\"out: \")\n",
    "print(netw.evaluate(X).round(3))\n",
    "print(\"\\nMSE: \")\n",
    "print(netw.loss_MSE(X, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35c6dc",
   "metadata": {},
   "source": [
    "### P4.18: Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84756e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "netw = Netwerk(1, 4, 4, 3, activatie_functies.SIGMOID(), .1)\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "def tobin(x):\n",
    "    #hiermee transformeren wij de target zo dat er voor elke bloem een neuron is dus bijv 1 -> [0 1 0]\n",
    "    res = np.zeros(3)\n",
    "    res[x] = 1\n",
    "    return res\n",
    "target = np.array(list(map(tobin, iris.target)))\n",
    "\n",
    "for i in range(1000):\n",
    "    netw.update_backprop(X, target)\n",
    "\n",
    "print(\"out: \")\n",
    "print(netw.evaluate(X).round(3))\n",
    "print(\"\\nMSE: \")\n",
    "print(netw.loss_MSE(X, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265c861",
   "metadata": {},
   "source": [
    "### P4.19: Digit dataset\n",
    "\n",
    "onderaan een aantal van de digits met bijhorende voorspelling. \n",
    "1-vormige cijfers lijkt het model het meest moeite mee te hebben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "netw = Netwerk(2, 15, 64, 10, activatie_functies.SIGMOID(), 0.1)\n",
    "digits = load_digits()\n",
    "data = np.array(digits['data'])\n",
    "\n",
    "def tobin(x):\n",
    "    #hetzelfde als bij de bloemen, maar dan voor elke digit\n",
    "    res = np.zeros(10) \n",
    "    res[x] = 1\n",
    "    return res\n",
    "target = np.array(list(map(tobin, digits['target'])))\n",
    "\n",
    "#ongeveer 20% van de dataset apart houden voor validatie\n",
    "data_train = data[:-360] \n",
    "data_test = data[-360:]\n",
    "\n",
    "target_train = target[:-360]\n",
    "target_test = target[-360:]\n",
    "\n",
    "for i in range(100):\n",
    "    netw.update_backprop(data_train, target_train)\n",
    "\n",
    "#de accuracy is simpelweg het aantaL correcte voorspellingen gedeeld door het totaal aantal voorspellingen.\n",
    "#ik heb dit hier gebruikt omdat ik dat representatiever vind voor het functioneren van het model.\n",
    "acc = (digits['target'][-360:] == list(map(np.argmax, netw.evaluate(data_test)))).astype(int).mean()\n",
    "print(\"\\naccuracy test set: \" + str(acc))\n",
    "print(\"MSE test set:\\n\" + str(netw.loss_MSE(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1966471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = list(map(np.argmax, netw.evaluate(data_test[:100])))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(digits.images[-360:][i], cmap=plt.cm.gray_r)\n",
    "    plt.title(str(res[i]))\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c7e42",
   "metadata": {},
   "source": [
    "### Bronnen\n",
    " - https://en.wikipedia.org/wiki/Gradient_descent\n",
    " - https://en.wikipedia.org/wiki/Backpropagation\n",
    " - https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
